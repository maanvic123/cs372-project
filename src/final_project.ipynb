{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maanvic123/cs372-project/blob/main/final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAQpzxkyuRPc"
      },
      "source": [
        "# **Notebook Configuration**\n",
        "1.   Install dependencies and libraries.\n",
        "2.   Load input files and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9TNQwDPRxgJ"
      },
      "outputs": [],
      "source": [
        "# install dependencies and libraries\n",
        "\n",
        "!pip install -q openai faiss-cpu numpy joblib requests tqdm h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuGeIPfOAa5w"
      },
      "outputs": [],
      "source": [
        "# get API keys\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "SPOTIFY_CLIENT_ID = userdata.get('SPOTIFY_CLIENT_ID')\n",
        "SPOTIFY_CLIENT_SECRET = userdata.get('SPOTIFY_CLIENT_SECRET')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnSpe2K32WeN",
        "outputId": "0f2f37f1-9a3d-47cc-8469-119305b1a259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRH6xARkR6OY",
        "outputId": "39803b1c-9d47-4d2a-a662-e348073fbc4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/project\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "# clone and pull code files from git repo\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = Path(\"/content/project\")\n",
        "REPO_URL = \"https://github.com/maanvic123/cs372-project.git\"\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "  !git clone {REPO_URL} {REPO_DIR}\n",
        "else:\n",
        "  %cd {REPO_DIR}\n",
        "  !git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rddIejX4zKkX",
        "outputId": "7afde986-c3b0-488e-c6ed-8bd7abba319a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raw dataset loaded\n",
            "preprocessing artifacts loaded\n",
            "X shape: (1157421, 14)\n",
            "num track IDs: 1157421\n"
          ]
        }
      ],
      "source": [
        "# load data and preprocessing files from git repo and google drive\n",
        "from pathlib import Path\n",
        "import os, shutil, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# set filepaths\n",
        "COLAB_DIR = Path('/content/project')\n",
        "\n",
        "DRIVE_DIR = Path(\"/content/drive/MyDrive/DUKE | 2022 - 2026/2526 -- senior/CS 372 - Intro to Applied Machine Learning/372 final project\")\n",
        "raw_data_csv = Path(DRIVE_DIR / \"raw_spotify_data.csv\")\n",
        "\n",
        "ARTIFACTS_DIR = Path(\"/content/project/data/processed\")\n",
        "features_path = Path(ARTIFACTS_DIR / \"features.npy\")\n",
        "track_ids_path = Path(ARTIFACTS_DIR / \"track_ids.npy\")\n",
        "scaler_path = Path(ARTIFACTS_DIR / \"scaler.pkl\")\n",
        "\n",
        "# load raw csv data file\n",
        "df_raw = pd.read_csv(raw_data_csv)\n",
        "print(\"raw dataset loaded\")\n",
        "\n",
        "# load preprocessing artifacts\n",
        "X = np.load(features_path)\n",
        "track_ids = np.load(track_ids_path, allow_pickle=True).astype(str)\n",
        "scaler = joblib.load(str(scaler_path))\n",
        "print(\"preprocessing artifacts loaded\")\n",
        "\n",
        "# check files\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"num track IDs:\", len(track_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-7sydze5BgR"
      },
      "source": [
        "# **Create Textual Embeddings of Songs**\n",
        "1.   Convert numerical audio features to words based on ranges.\n",
        "2.   Add \"description\" column to concatenate all text audio features.\n",
        "3. Use OpenAI \"text-embedding-3-small\" model to create textual embeddings of songs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEj97vb724oj",
        "outputId": "bfa39b27-dfe4-408e-dba0-16d53eaf726e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computed and wrote 1157421 descriptions for songs in dataset.\n",
            "examples: [\"I Won't Give Up by Jason Mraz is a track from the acoustic genre that feels very sad, relaxed, danceable, mostly acoustic, fast tempo\", '93 Million Miles by Jason Mraz is a track from the acoustic genre that feels netural, relaxed, danceable, mostly acoustic, fast tempo', 'Do Not Let Me Go by Joshua Hyslop is a track from the acoustic genre that feels very sad, calm, danceable, electronic or produced, fast tempo']\n"
          ]
        }
      ],
      "source": [
        "# build description column using audio features - map numerical values to textual descriptions\n",
        "\n",
        "from math import isnan\n",
        "\n",
        "# valence (0.0-1.0) - measure of track's happiness or positiveness\n",
        "def valence_word(v):\n",
        "  if v is None: return \"neutral\"\n",
        "  v = float(v)\n",
        "  if v < 0.25: return \"very sad\"\n",
        "  if v < 0.45: return \"sad\"\n",
        "  if v < 0.55: return \"netural\"\n",
        "  if v < 0.75: return \"happy\"\n",
        "  return \"very happy\"\n",
        "\n",
        "# energy (0.0-1.0) - track's perceptual intensity, loudness, and activity\n",
        "def energy_word(e):\n",
        "  if e is None: return \"moderate energy\"\n",
        "  e = float(e)\n",
        "  if e < 0.25: return \"calm\"\n",
        "  if e < 0.5: return \"relaxed\"\n",
        "  if e < 0.75: return \"energetic\"\n",
        "  return \"very energetic\"\n",
        "\n",
        "# danceability (0.0-1.0) - how suitable a track is for dancing, based on tempo, beat strength, stability, and regularity\n",
        "def danceability_word(d):\n",
        "  if d is None: return \"unknown danceability\"\n",
        "  d = float(d)\n",
        "  if d < 0.35: return \"not very danceable\"\n",
        "  if d < 0.7: return \"danceable\"\n",
        "  return \"highly danceable\"\n",
        "\n",
        "# acoustic (0.0-1.0) - how likely a track is to be acoustic (natural, organic sounds like guitars, piano) versus electronic\n",
        "def acousticness_word(a):\n",
        "  if a is None: return \"mixed\"\n",
        "  a = float(a)\n",
        "  if a > 0.7: return \"acoustic\"\n",
        "  if a > 0.4: return \"mostly acoustic\"\n",
        "  return \"electronic or produced\"\n",
        "\n",
        "# tempo (BPM) - speed or pace of the track\n",
        "def tempo_word(t):\n",
        "  if t is None: return \"moderate tempo\"\n",
        "  t = float(t)\n",
        "  if t < 80: return \"slow tempo\"\n",
        "  if t < 120: return \"mid tempo\"\n",
        "  return \"fast tempo\"\n",
        "\n",
        "\n",
        "# compute word columns for audio features\n",
        "df_raw[\"valence_word\"] = df_raw[\"valence\"].apply(valence_word)\n",
        "df_raw[\"energy_word\"] = df_raw[\"energy\"].apply(energy_word)\n",
        "df_raw[\"danceability_word\"] = df_raw[\"danceability\"].apply(danceability_word)\n",
        "df_raw[\"acousticness_word\"] = df_raw[\"acousticness\"].apply(acousticness_word)\n",
        "df_raw[\"tempo_word\"] = df_raw[\"tempo\"].apply(tempo_word)\n",
        "\n",
        "\n",
        "# put words together to create textual description for song\n",
        "def write_description(track):\n",
        "  desc_parts = []\n",
        "\n",
        "  # get and append textual features to description\n",
        "  title = str(track.get(\"track_name\", \"\")).strip()\n",
        "  artist = str(track.get(\"artist_name\", \"\")).strip()\n",
        "  genre = str(track.get(\"genre\", \"\")).strip()\n",
        "\n",
        "  desc_parts.append(f\"{title} by {artist} is a track\")\n",
        "  if genre != \"\":\n",
        "    desc_parts.append(f\"from the {genre} genre\")\n",
        "\n",
        "  # get and append audio features to description\n",
        "  valence_word = str(track.get(\"valence_word\", \"\"))\n",
        "  energy_word = str(track.get(\"energy_word\", \"\"))\n",
        "  danceability_word = str(track.get(\"danceability_word\", \"\"))\n",
        "  acousticness_word = str(track.get(\"acousticness_word\", \"\"))\n",
        "  tempo_word = str(track.get(\"tempo_word\", \"\"))\n",
        "\n",
        "  desc_parts.append(\"that feels\")\n",
        "  desc_parts.append(f\"{valence_word}, {energy_word}, {danceability_word}, {acousticness_word}, {tempo_word}\")\n",
        "\n",
        "  return \" \".join(desc_parts)\n",
        "\n",
        "\n",
        "# compute and add description column to dataset\n",
        "df_raw[\"description\"] = df_raw.apply(write_description, axis=1)\n",
        "\n",
        "# save computed description texts to npy array\n",
        "description_map = dict(zip(df_raw[\"track_id\"].astype(str), df_raw[\"description\"].astype(str)))\n",
        "description_texts = [description_map.get(tid, str(tid)) for tid in track_ids.tolist()]\n",
        "np.save(Path(DRIVE_DIR / \"description_texts.npy\"), np.array(description_texts, dtype=object))\n",
        "\n",
        "print(f\"computed and wrote {len(description_texts)} descriptions for songs in dataset.\")\n",
        "print(\"examples:\", description_texts[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "QtPy1eCG5uqe",
        "outputId": "e9806883-090c-4af1-a265-8906148a8edf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finished all embeddings!\n",
            "shape of dataset of song embeddings: (1157421, 1536)\n",
            "example embedding: [-0.0170249   0.04692689 -0.02582091 ... -0.01914884  0.03994346\n",
            " -0.01601297]\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_8ccf54b1-6740-4d14-bfbc-b6ec60976850\", \"song_text_embeddings.h5\", 2253375565)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings downloaded to computer\n"
          ]
        }
      ],
      "source": [
        "# create textual embeddings of songs with OpenAI text-embedding-3-small model\n",
        "\n",
        "import openai, time, math\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "# init openAI client\n",
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# set openAI textual embedding model and hyperparameters\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "OUT_EMBEDDING_PATH = COLAB_DIR / 'song_text_embeddings.h5'\n",
        "BATCH_SIZE = 512\n",
        "EMBEDDING_DIM = 1536\n",
        "\n",
        "# load description_texts file\n",
        "description_texts = np.load(Path(DRIVE_DIR / \"description_texts.npy\"), allow_pickle=True)\n",
        "\n",
        "\n",
        "# function to get textual embeddings for 1 batch - batch_texts: list[str] -> numpy array (len(batch_texts), EMBEDDING_DIM)\n",
        "def get_openai_embedding(batch_texts):\n",
        "  # Use the new client.embeddings.create syntax\n",
        "  res = client.embeddings.create(model=EMBEDDING_MODEL, input=batch_texts)\n",
        "  return [item.embedding for item in res.data]\n",
        "\n",
        "\n",
        "# write embeddings to h5 file if not already computed\n",
        "if not OUT_EMBEDDING_PATH.exists():\n",
        "  n = len(description_texts)\n",
        "\n",
        "  with h5py.File(str(OUT_EMBEDDING_PATH), \"w\") as f:\n",
        "    # create dataset\n",
        "    dset = f.create_dataset(\n",
        "      \"embeddings\",\n",
        "      shape=(n, EMBEDDING_DIM),\n",
        "      dtype=\"float32\",\n",
        "      compression=\"gzip\",\n",
        "      chunks=(min(BATCH_SIZE, n), EMBEDDING_DIM))\n",
        "\n",
        "    # embedding loop\n",
        "    for i in tqdm(range(0, n, BATCH_SIZE), desc=\"Embedding\"):\n",
        "      # define and get embedding for batch\n",
        "      batch = description_texts[i : i + BATCH_SIZE]\n",
        "      batch_embeddings = get_openai_embedding(batch)\n",
        "      batch_arr = np.array(batch_embeddings, dtype=\"float32\")\n",
        "\n",
        "      # write batch embedding to dataset\n",
        "      dset[i : i + batch_arr.shape[0], :] = batch_arr\n",
        "\n",
        "# all embeddings computed\n",
        "print(\"finished all embeddings!\")\n",
        "\n",
        "# print shape of embeddings (1157421 song embeddings, 1536 embedding dimension)\n",
        "with h5py.File(OUT_EMBEDDING_PATH, 'r') as f:\n",
        "  if 'embeddings' in f:\n",
        "    dataset = f['embeddings']\n",
        "    print(f\"shape of dataset of song embeddings: {dataset.shape}\")\n",
        "\n",
        "    # verify embeddings\n",
        "    print(f\"example embedding: {dataset[0]}\")\n",
        "\n",
        "# download embeddings file to local computer\n",
        "files.download(OUT_EMBEDDING_PATH)\n",
        "print(\"embeddings downloaded to computer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbvgpLJRsnaP"
      },
      "source": [
        "# **Similarity Search (FAISS)**\n",
        "*Find vectors (tracks) with highest similarity to query vector (user-inputted mood embedding) from 1M+ vectors (tracks) using FAISS (Facebook AI Similarity Search).*\n",
        "\n",
        "1. After L2-normalizing song numerical vectors, cosine similarity is the same as dot product: cosine(x,y) = dot(x,y). FAISS supports fast, efficient dot product computation.\n",
        "2. FAISS organizes songs into groups (\"centroids\") based on vector similarity.\n",
        "3. Given user-inputted mood embedding, FAISS searches within the most relevant groups for specific song vectors to output as recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "QtG93lu_ZcAv",
        "outputId": "a05e8b39-ac61-4e33-f8c0-e96253f112de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings L2-normalized successfully\n",
            "norm of first embedding after normalization: 1.00\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_466cf094-878b-4e96-9dc9-8e339a17e8ae\", \"song_text_embeddings.h5\", 2264044875)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "normalized embeddings downloaded to computer\n"
          ]
        }
      ],
      "source": [
        "# compute L2 normalizations of song embeddings in blocks of vectors\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import cupy as cp\n",
        "\n",
        "EMBEDDINGS_PATH = COLAB_DIR / 'song_text_embeddings.h5'\n",
        "\n",
        "# block size - load 10k vectors at a time for normalization\n",
        "BLOCK_SIZE = 10000\n",
        "\n",
        "with h5py.File(str(EMBEDDINGS_PATH), \"r+\") as f:\n",
        "  # get embeddings dataset from h5py file\n",
        "  dset = f[\"embeddings\"]\n",
        "  N, D = dset.shape\n",
        "\n",
        "  # normalize vectors in blocks of BLOCK_SIZE\n",
        "  for i in range(0, N, BLOCK_SIZE):\n",
        "    # load block of vectors from dataset and move to GPU\n",
        "    j = min(N, i + BLOCK_SIZE)\n",
        "    X = dset[i:j].astype(np.float32)\n",
        "    X = cp.asarray(X)\n",
        "\n",
        "    # calculate L2 norm for each vector in block\n",
        "    norms = cp.linalg.norm(X, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1.0\n",
        "    X = X / norms\n",
        "\n",
        "    # move normalized block back to CPU and write to dataset\n",
        "    X = cp.asnumpy(X)\n",
        "    dset[i:j] = X\n",
        "\n",
        "  f.flush()\n",
        "\n",
        "print(\"embeddings L2-normalized successfully\")\n",
        "\n",
        "# verify normalizations (all should be 1.0)\n",
        "with h5py.File(str(EMBEDDINGS_PATH), \"r\") as f:\n",
        "  print(f\"norm of first embedding after normalization: {np.linalg.norm(f[\"embeddings\"][0]):.2f}\")\n",
        "\n",
        "# download normalized embeddings to computer\n",
        "files.download(EMBEDDINGS_PATH)\n",
        "print(\"normalized embeddings downloaded to computer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "mOyycaReXgq-",
        "outputId": "c8148742-5323-4226-e650-7aa0975df735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training sample shape: (100000, 1536)\n",
            "FAISS index trained and saved to /content/project/faiss_ivf_flat.index\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_fe22df38-2e1b-4469-b6d7-3a9717a2538b\", \"faiss_ivf_flat.index\", 7170851315)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index downloaded to computer\n"
          ]
        }
      ],
      "source": [
        "# train FAISS clustering model to efficiently find similar songs\n",
        "# 1. randomly sample some normalized embedding vectors to learn embedding space (identify centroids, how to divide embedding space)\n",
        "# 2. build FAISS index (IndexIVFFlat) for approx nearest neighbor (ANN) search: for each vector, find and store in appropriate centroid list\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import h5py\n",
        "\n",
        "# set file paths\n",
        "INDEX_PATH = COLAB_DIR / \"faiss_ivf_flat.index\"\n",
        "\n",
        "# FAISS clustering model training parameters\n",
        "nlist = 8192              # number of coarse clusters (centroids, groups)\n",
        "nprobe = 16               # how many clusters to search at query time\n",
        "TRAINING_SAMPLE = 100000  # training sample size (100k)\n",
        "BATCH_SIZE = 10000        # add vectors in 10k batches\n",
        "\n",
        "\n",
        "with h5py.File(str(EMBEDDINGS_PATH), \"r\") as f:\n",
        "  # load normalized embeddings\n",
        "  dset = f[\"embeddings\"]\n",
        "  N, D = dset.shape\n",
        "\n",
        "  # init inner product quantizer (to learn how to divide embedding space) & INVFFLat index (inverted file index with full vectors in each cluster list)\n",
        "  quantizer = faiss.IndexFlatIP(D)\n",
        "  index = faiss.IndexIVFFlat(quantizer, D, nlist, faiss.METRIC_INNER_PRODUCT)   # inner product ~ cosine similarity\n",
        "\n",
        "  # randomly sample training vectors to train index\n",
        "  random_seed = np.random.RandomState(seed=2025)\n",
        "  selection = random_seed.choice(N, size=min(TRAINING_SAMPLE, N), replace=False)\n",
        "  sample = np.empty((selection.shape[0], D), dtype=np.float32)\n",
        "  for i, pos in enumerate(selection):\n",
        "    sample[i] = dset[pos]\n",
        "  print(f\"training sample shape:\", sample.shape)\n",
        "\n",
        "  # train index with sample data - runs k-means to learn nlist centroids with quantizer\n",
        "  index.train(sample)\n",
        "\n",
        "  # add all vectors to index in batches\n",
        "  for i in range(0, N, BATCH_SIZE):            # for each vector, FAISS finds nearest centroid and puts in inverted list\n",
        "    j = min(N, i + BATCH_SIZE)\n",
        "    vectors = dset[i:j].astype(np.float32)\n",
        "    index.add(vectors)\n",
        "\n",
        "  # set query time search parameters\n",
        "  index.nprobe = nprobe\n",
        "\n",
        "  # save trained FAISS index\n",
        "  faiss.write_index(index, str(INDEX_PATH))\n",
        "  print(\"FAISS index trained and saved to\", INDEX_PATH)\n",
        "\n",
        "  # download FAISS index to computer\n",
        "  files.download(INDEX_PATH)\n",
        "  print(\"FAISS index downloaded to computer\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPRr7LZhaSYYug6MPLkERYh",
      "gpuType": "A100",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
